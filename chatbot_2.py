import streamlit as st
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
import tempfile
import os
from pathlib import Path

# Konfiguracja strony
st.set_page_config(
    page_title="RAG Chatbot",
    page_icon="ğŸ“š",
    layout="wide"
)

st.title("ğŸ“š RAG Chatbot - Zapytaj o swoje dokumenty")
st.markdown("---")

# Inicjalizacja session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vectordb" not in st.session_state:
    st.session_state.vectordb = None

if "documents_loaded" not in st.session_state:
    st.session_state.documents_loaded = False


# Funkcja do przetwarzania dokumentÃ³w
def process_documents(uploaded_files, api_key, chunk_size, chunk_overlap):
    """Przetwarza zaÅ‚adowane pliki PDF i tworzy bazÄ™ wektorowÄ…"""
    with st.spinner("ğŸ“– ÅadujÄ™ i przetwarszam dokumenty..."):
        try:
            all_docs = []

            # Przetwarzanie kaÅ¼dego pliku
            for uploaded_file in uploaded_files:
                # Zapisz plik tymczasowo
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    tmp_path = tmp_file.name

                # ZaÅ‚aduj PDF
                loader = PyPDFLoader(tmp_path)
                pages = loader.load()

                st.info(f"ğŸ“„ {uploaded_file.name}: zaÅ‚adowano {len(pages)} stron")

                # Konwertuj na dokumenty
                for page in pages:
                    all_docs.append(
                        Document(
                            page_content=page.page_content,
                            metadata={
                                **page.metadata,
                                "filename": uploaded_file.name
                            }
                        )
                    )

                # UsuÅ„ tymczasowy plik
                os.unlink(tmp_path)

            # Podziel dokumenty na chunki
            splitter = CharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            documents_split = splitter.split_documents(all_docs)

            st.info(f"âœ‚ï¸ Podzielono na {len(documents_split)} fragmentÃ³w")

            # UtwÃ³rz embeddingi i bazÄ™ wektorowÄ…
            embeddings = OpenAIEmbeddings(api_key=api_key)

            vectordb = Chroma.from_documents(
                documents=documents_split,
                embedding=embeddings,
                collection_name="rag_chatbot"
            )

            # Zapisz w session state
            st.session_state.vectordb = vectordb
            st.session_state.documents_loaded = True
            st.session_state.api_key = api_key

            st.success(f"âœ… Dokumenty przetworzone! Zaindeksowano {len(all_docs)} stron z {len(uploaded_files)} plikÃ³w.")

        except Exception as e:
            st.error(f"âŒ BÅ‚Ä…d podczas przetwarzania: {str(e)}")


# Funkcja do generowania odpowiedzi RAG
def generate_rag_response(question, api_key, model_name, temperature, k_results):
    """Generuje odpowiedÅº uÅ¼ywajÄ…c RAG"""
    try:
        # Inicjalizacja modelu
        llm = ChatOpenAI(
            api_key=api_key,
            temperature=temperature,
            model=model_name
        )

        # Template promptu
        template = """
Odpowiedz na pytanie na podstawie dostarczonego kontekstu z dokumentÃ³w.

Kontekst:
{kontekst}

Pytanie: {pytanie}

Instrukcje:
- Odpowiadaj wyÅ‚Ä…cznie na podstawie dostarczonego kontekstu
- JeÅ›li nie znajdziesz odpowiedzi w kontekÅ›cie, powiedz to wprost
- Cytuj konkretne fragmenty z dokumentÃ³w jeÅ›li to moÅ¼liwe
- BÄ…dÅº precyzyjny i zwiÄ™zÅ‚y

OdpowiedÅº:
"""

        prompt = PromptTemplate.from_template(template=template)

        # Retriever
        retrieval = st.session_state.vectordb.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k_results}
        )

        # ÅaÅ„cuch RAG
        chain = (
                {
                    "kontekst": retrieval,
                    "pytanie": RunnablePassthrough()
                }
                | prompt
                | llm
                | StrOutputParser()
        )

        # Generuj odpowiedÅº
        response = chain.invoke(question)

        # Pobierz dokumenty ÅºrÃ³dÅ‚owe
        source_docs = retrieval.invoke(question)

        return response, source_docs

    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas generowania odpowiedzi: {str(e)}")
        return None, None


# Sidebar z konfiguracjÄ…
with st.sidebar:
    st.header("âš™ï¸ Konfiguracja")

    api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="WprowadÅº swÃ³j klucz API OpenAI"
    )

    st.subheader("ğŸ“„ ZaÅ‚aduj dokumenty PDF")
    uploaded_files = st.file_uploader(
        "Wybierz pliki PDF",
        type=['pdf'],
        accept_multiple_files=True,
        help="MoÅ¼esz zaÅ‚adowaÄ‡ wiele plikÃ³w PDF"
    )

    st.subheader("ğŸ”§ Parametry RAG")

    chunk_size = st.slider(
        "Rozmiar chunka",
        min_value=256,
        max_value=2048,
        value=1024,
        step=128,
        help="Rozmiar pojedynczego fragmentu tekstu"
    )

    chunk_overlap = st.slider(
        "NakÅ‚adanie chunkÃ³w",
        min_value=0,
        max_value=200,
        value=50,
        step=10,
        help="Ile znakÃ³w ma siÄ™ nakÅ‚adaÄ‡ miÄ™dzy chunkami"
    )

    k_results = st.slider(
        "Liczba wynikÃ³w wyszukiwania",
        min_value=1,
        max_value=10,
        value=3,
        help="Ile najlepszych fragmentÃ³w pobraÄ‡ z bazy"
    )

    st.subheader("ğŸ¤– Parametry modelu")

    model_name = st.selectbox(
        "Model",
        ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
        index=0
    )

    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=0.1,
        help="WyÅ¼sza wartoÅ›Ä‡ = bardziej kreatywne odpowiedzi"
    )

    st.markdown("---")

    if st.button("ğŸ”„ PrzetwÃ³rz dokumenty", type="primary"):
        if not api_key:
            st.error("âš ï¸ WprowadÅº klucz API OpenAI!")
        elif not uploaded_files:
            st.error("âš ï¸ ZaÅ‚aduj przynajmniej jeden plik PDF!")
        else:
            process_documents(uploaded_files, api_key, chunk_size, chunk_overlap)

    if st.button("ğŸ—‘ï¸ WyczyÅ›Ä‡ historiÄ™ czatu"):
        st.session_state.messages = []
        st.rerun()

    if st.button("ğŸ”„ Resetuj bazÄ™ dokumentÃ³w"):
        st.session_state.vectordb = None
        st.session_state.documents_loaded = False
        st.session_state.messages = []
        st.success("âœ… Baza dokumentÃ³w zostaÅ‚a zresetowana")
        st.rerun()

# Layout gÅ‚Ã³wny - dwie kolumny
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ’¬ Czat")

    # WyÅ›wietlanie historii konwersacji
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # WyÅ›wietl ÅºrÃ³dÅ‚a jeÅ›li sÄ… dostÄ™pne
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("ğŸ“š Å¹rÃ³dÅ‚a"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Å¹rÃ³dÅ‚o {i}:**")
                        st.markdown(f"*Plik: {source.metadata.get('filename', 'N/A')}*")
                        st.markdown(f"*Strona: {source.metadata.get('page', 'N/A')}*")
                        st.text(source.page_content[:300] + "...")
                        st.markdown("---")

    # Input uÅ¼ytkownika
    if not st.session_state.documents_loaded:
        st.info("ğŸ‘† ZaÅ‚aduj dokumenty PDF w sidebarze, aby rozpoczÄ…Ä‡ czat")
    else:
        if prompt := st.chat_input("Zadaj pytanie o dokumenty..."):
            # SprawdÅº API key
            if not api_key:
                st.error("âš ï¸ WprowadÅº klucz API OpenAI w sidebarze!")
                st.stop()

            # Dodaj wiadomoÅ›Ä‡ uÅ¼ytkownika
            st.session_state.messages.append({"role": "user", "content": prompt})

            with st.chat_message("user"):
                st.markdown(prompt)

            # Generuj odpowiedÅº
            with st.chat_message("assistant"):
                with st.spinner("ğŸ” Szukam w dokumentach..."):
                    response, sources = generate_rag_response(
                        prompt,
                        api_key,
                        model_name,
                        temperature,
                        k_results
                    )

                    if response:
                        st.markdown(response)

                        # Dodaj odpowiedÅº do historii
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response,
                            "sources": sources
                        })

                        # WyÅ›wietl ÅºrÃ³dÅ‚a
                        if sources:
                            with st.expander("ğŸ“š Å¹rÃ³dÅ‚a"):
                                for i, source in enumerate(sources, 1):
                                    st.markdown(f"**Å¹rÃ³dÅ‚o {i}:**")
                                    st.markdown(f"*Plik: {source.metadata.get('filename', 'N/A')}*")
                                    st.markdown(f"*Strona: {source.metadata.get('page', 'N/A')}*")
                                    st.text(source.page_content[:300] + "...")
                                    st.markdown("---")

with col2:
    st.subheader("ğŸ“Š Status")

    if st.session_state.documents_loaded:
        st.success("âœ… Dokumenty zaÅ‚adowane")

        # Statystyki
        if st.session_state.vectordb:
            st.metric("FragmentÃ³w w bazie",
                      st.session_state.vectordb._collection.count())
    else:
        st.warning("â³ Brak zaÅ‚adowanych dokumentÃ³w")

    st.markdown("---")
    st.subheader("â„¹ï¸ Jak uÅ¼ywaÄ‡?")
    st.markdown("""
    1. WprowadÅº klucz API OpenAI
    2. ZaÅ‚aduj pliki PDF
    3. Kliknij "PrzetwÃ³rz dokumenty"
    4. Zadawaj pytania o zawartoÅ›Ä‡ dokumentÃ³w
    """)

    st.markdown("---")
    st.subheader("ğŸ¯ PrzykÅ‚adowe pytania")
    st.markdown("""
    - Jakie sÄ… gÅ‚Ã³wne tematy dokumentu?
    - Czy w dokumencie jest informacja o...?
    - Podsumuj sekcjÄ™ dotyczÄ…cÄ…...
    - Jakie sÄ… kluczowe daty/liczby?
    """)

# Stopka
st.markdown("---")
st.caption("ğŸ’¡ RAG (Retrieval Augmented Generation) pozwala chatbotowi odpowiadaÄ‡ na podstawie Twoich dokumentÃ³w")
